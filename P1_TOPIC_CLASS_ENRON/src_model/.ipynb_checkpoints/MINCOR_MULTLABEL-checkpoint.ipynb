{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MINCOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Name: MINCOR Project\n",
    "Program Name: MINCOR_MULTI_LABEL\n",
    "Program Descriptions: Using the email content to predict which team to sent to \n",
    "Process:\n",
    "1. Read in data from the convert image \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a df to capture all text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc(ClassificationInterpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess all the files and store the content in a single csv\n",
    "* Create a single dataframe with   \n",
    "Y - [\"ASSIGN TO\", \"LEAD\", \"PRIORITY\"]  \n",
    "X - [\"TEXT\", \"DATE\", \"EMAIL\", \"SENDER\", ...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Frame 1: Filename and Text Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "def reduceTextSize(txt, skip_summarized = True):\n",
    "    isfailed  = 0\n",
    "    \n",
    "    # Get rid of dup ., dup line break and strange symbols\n",
    "    txt_A_Z = re.sub('\\n+','\\n',re.sub('\\.+','.',re.sub('[^A-Za-z0-9;:.\\- _\\n\\']+', '', str(txt))))\n",
    "    \n",
    "    # Try to clip the by word count    \n",
    "    txt_clipped = \" \".join(txt_A_Z.split(\" \")[0:800])\n",
    "    \n",
    "    # Try to reduce the size by summarize\n",
    "    txt_summarized = txt_clipped\n",
    "    \n",
    "    if not skip_summarized:\n",
    "        try:        \n",
    "            txt_summarized = summarize(txt_clipped, word_count=400)       \n",
    "            if len(txt_summarized) < 1:\n",
    "                txt_summarized = txt_clipped\n",
    "                isfailed = 1\n",
    "        except:            \n",
    "            isfailed = 1\n",
    "\n",
    "    return txt_summarized, isfailed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os \n",
    "\n",
    "srcs = os.listdir(path.joinpath(\"TXT_incoming\"))\n",
    "text = []\n",
    "i = 0\n",
    "cntfailed = 0\n",
    "for fp in srcs:\n",
    "    i = i + 1\n",
    "    if i % 2000 == 0:\n",
    "        print (str(cntfailed) + \":\" + str(i) + \"/\" + str(len(srcs)))\n",
    "    with open (path.joinpath(\"TXT_incoming\", fp), encoding='latin1') as file:\n",
    "        buf, isfailed = reduceTextSize(file.read())\n",
    "        cntfailed = cntfailed + isfailed\n",
    "        text.append(buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = [e.split(\".\")[0] for e in srcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_fname = pd.DataFrame({\"filename\":fname, \"text\":text})\n",
    "df_text_fname.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Frame 2: Filename and MASTER_PROOTID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* INC_ROOTID_ADOC.csv is supplied from James  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_inc = path.joinpath(\"config\", \"INC_ROOTID_ADOC.csv\")\n",
    "fp_inc = pd.read_csv(fn_inc, encoding = \"latin1\")\n",
    "fp_inc_part = fp_inc[fp_inc.FILESUFFIX == \"PDF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(fp_inc_part.MASTER_ROOTID)))\n",
    "print(len(fp_inc_part))\n",
    "\n",
    "fp_inc_part.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* INC_ADOC_FNAME.csv is supplied from Jason  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_inc_adoc = path.joinpath(\"config\", \"INC_ADOC_FNAME.csv\")\n",
    "fp_inc_adoc = pd.read_csv(fn_inc_adoc, encoding = \"latin1\")\n",
    "fp_inc_adoc[\"fname\"] = [e.split(\".\")[0] for e in fp_inc_adoc.filename]\n",
    "fp_inc_adoc = fp_inc_adoc[['DOC#','fname']]\n",
    "fp_inc_adoc.drop_duplicates(subset=['DOC#', 'fname'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(fp_inc_adoc[\"fname\"])))\n",
    "print(len(fp_inc_adoc))\n",
    "fp_inc_adoc.columns = ['ADOC_REF', 'filename']\n",
    "\n",
    "fp_inc_adoc.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_merge_inc = pd.merge(fp_inc_part, fp_inc_adoc, on='ADOC_REF', how='inner')\n",
    "fp_merge_inc = fp_merge_inc.groupby(['MASTER_ROOTID']).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fp_merge_inc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_merge_inc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Frame 3: MASTER_PROOTID and TREC's attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_assign = pd.read_csv(path.joinpath(\"TRECs\", '20190417-QTE_data_dump_assign.csv'), encoding = \"latin1\")\n",
    "df_client = pd.read_csv(path.joinpath(\"TRECs\", '20190417-QTE_data_dump_client.csv'), encoding = \"latin1\")\n",
    "df_master = pd.read_csv(path.joinpath(\"TRECs\", '20190417-QTE_data_dump_master.csv'), encoding = \"latin1\")\n",
    "df_master_client_relations = pd.read_csv(path.joinpath(\"TRECs\", '20190417-QTE_data_dump_master_client_relations.csv'), encoding = \"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_client.head(5)\n",
    "df_client['FNAME'] = df_client['FNAME'].astype(str).str.replace('nan',\"\")\n",
    "df_client['LNAME'] = df_client['LNAME'].astype(str).str.replace('nan',\"\")\n",
    "df_client['PROV'] = df_client['PROV'].astype(str).str.replace('nan',\"\")\n",
    "df_client['EMAIL'] = df_client['EMAIL'].astype(str).str.replace('nan',\"\")\n",
    "df_client['TEL1'] = df_client['TEL1'].astype(str).str.replace('nan',\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assign.master_rootid = df_assign.master_rootid.astype(int)\n",
    "df_assign['ASGNTO'] = df_assign['ASGNTO'].astype(str)\n",
    "df_assign.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master.ROOTID = df_master.ROOTID.astype(int)\n",
    "df_master.RECEIVED =pd.to_datetime(df_master.RECEIVED)\n",
    "df_master['LEAD'] = df_master['LEAD'].astype(str)\n",
    "df_master['PRIORITY'] = df_master['PRIORITY'].astype(str)\n",
    "df_master['SUBJECT'] = df_master['SUBJECT'].astype(str)\n",
    "\n",
    "df_master.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master_client_relations.master_rootid = df_master_client_relations.master_rootid.astype(int)\n",
    "df_master_client_relations.client_detailid = df_master_client_relations.client_detailid.astype(int)\n",
    "df_master_client_relations.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assign_grouped = df_assign.groupby(['master_rootid'])['ASGNTO'].apply(lambda x: ','.join(x)).reset_index()\n",
    "#DETAILID \tFNAME \tLNAME \tPROV \tEMAIL \tTEL1 \tmaster_rootid \tclient_detailid\n",
    "\n",
    "df_masterid_client = pd.merge(df_master_client_relations, df_client, left_on='client_detailid', right_on=\"DETAILID\", how='inner')\n",
    "\n",
    "df_masterid_FNAME = df_masterid_client.groupby(['master_rootid'])['FNAME'].apply(lambda x: ';'.join(x)).reset_index()\n",
    "df_masterid_FNAME.FNAME = df_masterid_FNAME.FNAME.str.split(';')\n",
    "\n",
    "df_masterid_LNAME = df_masterid_client.groupby(['master_rootid'])['LNAME'].apply(lambda x: ';'.join(x)).reset_index()\n",
    "df_masterid_LNAME.LNAME = df_masterid_LNAME.LNAME.str.split(';')\n",
    "\n",
    "df_masterid_PROV = df_masterid_client.groupby(['master_rootid'])['PROV'].apply(lambda x: ';'.join(x)).reset_index()\n",
    "df_masterid_PROV.PROV = df_masterid_PROV.PROV.str.split(';')\n",
    "\n",
    "df_masterid_EMAIL = df_masterid_client.groupby(['master_rootid'])['EMAIL'].apply(lambda x: ';'.join(x)).reset_index()\n",
    "df_masterid_EMAIL.EMAIL = df_masterid_EMAIL.EMAIL.str.split(';')\n",
    "\n",
    "df_masterid_TEL1 = df_masterid_client.groupby(['master_rootid'])['TEL1'].apply(lambda x: ';'.join(x)).reset_index()\n",
    "df_masterid_TEL1.TEL1 = df_masterid_TEL1.TEL1.str.split(';')\n",
    "\n",
    "df_assign_grouped = df_assign.groupby(['master_rootid'])['ASGNTO'].apply(lambda x: ','.join(x)).reset_index()\n",
    "df_assign_grouped.ASGNTO = df_assign_grouped.ASGNTO.str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assign_grouped.ASGNTO.to_csv(\"check.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_assign_grouped.head(5)\n",
    "\n",
    "li = []\n",
    "for teams in df_assign_grouped.ASGNTO:\n",
    "    li = li + teams\n",
    "set(li)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(li))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "x = [len(e) for e in df_assign_grouped[\"ASGNTO\"]]\n",
    "plt.hist(x, 20,\n",
    "         density=True,\n",
    "         histtype='bar',\n",
    "         facecolor='b',\n",
    "         alpha=0.5)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(np.quantile(x, 0.1, axis=0))\n",
    "print(np.quantile(x, 0.25, axis=0))\n",
    "print(np.quantile(x, 0.5, axis=0))\n",
    "print(np.quantile(x, 0.75, axis=0))\n",
    "print(np.quantile(x, 0.90, axis=0))\n",
    "print(max(x))\n",
    "print(sum(x)/len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = df_master.rename(columns={'ROOTID': 'master_rootid'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "dfs = [df_master, df_assign_grouped, df_masterid_FNAME, df_masterid_LNAME, df_masterid_PROV, df_masterid_EMAIL, df_masterid_TEL1]\n",
    "df_base = reduce(lambda left,right: pd.merge(left,right,on='master_rootid'), dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"df_base length: \" + str(len(df_base)))\n",
    "df_base.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge 3 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_text_fname.columns)\n",
    "print(fp_merge_inc.columns)\n",
    "print(df_base.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = pd.merge(df_base, fp_merge_inc, left_on='master_rootid', right_on =\"MASTER_ROOTID\", how='left').merge(df_text_fname, on='filename', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eee = df_model.text[0]\n",
    "re.sub('\\n+','\\n',re.sub('\\.+','.',re.sub('[^A-Za-z0-9;:. \\n\\']+', '', eee)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_model[df_model.text.str.len() >= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_model))\n",
    "df_model.text.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 1000)\n",
    "df_model.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.to_csv(\"mincor.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model[df_model.text.str.len()>10].to_csv(path.joinpath(\"df_model.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "df_raw = pd.read_csv(path.joinpath(\"df_model.csv\"))\n",
    "df_raw['text'] = [' '.join(e.split()[:300]) for e in df_raw.text]\n",
    "\n",
    "#isQ = ['Q' if value % 2 == 0 else 'A' for value in df_raw.index]\n",
    "#df_raw['type'] = isQ\n",
    "\n",
    "df = df_raw[['ASGNTO', 'text', \"filename\"]]\n",
    "df.columns = ['label', 'text', \"filename\"]\n",
    "\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_valid = [random.randint(1,101) > 85 for value in df.index]\n",
    "df['is_valid'] = is_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(df.is_valid))\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path.joinpath('texts.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that language models can use a lot of GPU, so you may need to decrease batchsize here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs=24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's grab the full dataset for what follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./data\")\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews are in a training and test set following an imagenet structure. The only difference is that there is an `unsup` folder on top of `train` and `test` that contains the unlabelled data.\n",
    "\n",
    "We're not going to train a model that classifies the reviews from scratch. Like in computer vision, we'll use a model pretrained on a bigger dataset (a cleaned subset of wikipedia called [wikitext-103](https://einstein.ai/research/blog/the-wikitext-long-term-dependency-language-modeling-dataset)). That model has been trained to guess what the next word is, its input being all the previous words. It has a recurrent structure and a hidden state that is updated each time it sees a new word. This hidden state thus contains information about the sentence up to that point.\n",
    "\n",
    "We are going to use that 'knowledge' of the English language to build our classifier, but first, like for computer vision, we need to fine-tune the pretrained model to our particular dataset. Because the English of the reviews left by people on IMDB isn't the same as the English of wikipedia, we'll need to adjust the parameters of our model by a little bit. Plus there might be some words that would be extremely common in the reviews dataset but would be barely present in wikipedia, and therefore might not be part of the vocabulary the model was trained on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where the unlabelled data is going to be useful to us, as we can use it to fine-tune our model. Let's create our data object with the data block API (next line takes a few minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = (TextList.from_csv(path, 'texts.csv', cols='text')\n",
    "                .split_from_df(col=3)\n",
    "                .label_for_lm() \n",
    "                #We want to do a language model so we label accordingly\n",
    "                .databunch(bs=bs))\n",
    "data_lm.save('data_lm.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to use a special kind of `TextDataBunch` for the language model, that ignores the labels (that's why we put 0 everywhere), will shuffle the texts at each epoch before concatenating them all together (only for training, we don't shuffle for the validation set) and will send batches that read that text in order with targets that are the next word in the sentence.\n",
    "\n",
    "The line before being a bit long, we want to load quickly the final ids by using the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm = load_data(path, 'data_lm.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lm.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then put this in a learner object very easily with a model loaded with the pretrained weights. They'll be downloaded the first time you'll execute the following line and stored in `~/.fastai/models/` (or elsewhere if you specified different paths in your config file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot(skip_end=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(2, 1e-2, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fit_head')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fit_head');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To complete the fine-tuning, we can then unfeeze and launch a new training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(5, 1e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('fine_tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is our model? Well let's try to see what it predicts after a few given words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('fine_tuned');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = \"animal is our friend \"\n",
    "N_WORDS = 40\n",
    "N_SENTENCES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\".join(learn.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to save not only the model, but also its encoder, the part that's responsible for creating and updating the hidden state. For the next part, we don't care about the part that tries to guess the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save_encoder('fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a new data object that only grabs the labelled data and keeps those labels. Again, this line takes a bit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need to modified the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = pd.read_csv(path.joinpath('texts.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl = \"['SPB-PDA-SMLP', 'SPB', 'MINCOR-RET', 'SPB', 'SPB-PDA', 'MINCOR-ED', 'MINCOR-ED', 'MISB', 'MISB', 'MINCOR-RET', 'MINCOR', 'MISB-TAND', 'MINCOR-PR', 'MISB', 'MINCOR-ED', 'SPB-PDA']\"\n",
    "def castStr2List(lbl_str):\n",
    "    return ([w.replace(\"'\",'').replace(\"[\",'').replace(\"]\",\"\").replace(\" \",\"\") for w in lbl_str.split(\",\")])\n",
    "\n",
    "# get the set of department used in the samples\n",
    "def findDistinctList(label):\n",
    "    li = []\n",
    "    for e in label:\n",
    "        li = li + list(set(castStr2List(e)))\n",
    "    df_li = DataFrame()\n",
    "    df_li['dept'] = li\n",
    "\n",
    "    return(df_li.groupby(['dept']).size().reset_index(name='counts'))\n",
    "    \n",
    "df_lbl_unique = findDistinctList(df_tmp.label)\n",
    "df_lbl_unique = df_lbl_unique[df_lbl_unique.counts > 100]\n",
    "lbl_unique = list(df_lbl_unique['dept'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lbl_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = np.array([])\n",
    "cols = np.array([])\n",
    "\n",
    "for e in df_tmp.label:        \n",
    "    li = castStr2List(e)\n",
    "    cols = []\n",
    "    for ee in lbl_unique:\n",
    "        if ee in li:\n",
    "            cols = cols + [1]\n",
    "        else:\n",
    "            cols = cols + [0]\n",
    "    if len(rows) == 0:\n",
    "        rows = [cols]\n",
    "    else:\n",
    "        rows = np.vstack([rows,cols])\n",
    "\n",
    "df_labels = pd.DataFrame(rows)\n",
    "df_labels.columns = lbl_unique\n",
    "df_labels.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.merge(df_tmp, df_labels, left_index=True, right_index=True)\n",
    "df_out.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = (TextList.from_df(df_out, cols='text', vocab=data_lm.vocab)\n",
    "             .split_from_df(col=3)\n",
    "             .label_from_df(cols=lbl_unique , classes=lbl_unique)\n",
    "             .databunch(bs=bs))\n",
    "data_clas.save(path.joinpath('data_clas.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = load_data(path, 'data_clas.pkl', bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas.show_batch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create a model to classify those reviews and load the encoder we saved before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.6)\n",
    "learn.load_encoder('fine_tuned_enc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit_one_cycle(20, 1e-3, moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('first');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-2)\n",
    "learn.fit_one_cycle(4, slice(1e-3/(2.6**4),1e-3), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('second');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(-3)\n",
    "learn.fit_one_cycle(4, slice(1e-4/(2.6**4),1e-4), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('third');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(6, slice(5e-5/(2.6**4),5e-5), moms=(0.8,0.7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.predict(\n",
    "    \"\"\"\n",
    "Cheslock Maril n QtE Sent: Subject: Message Email Name Cityrr own cgelder2002yahoo.com June-04-18 8:41 PM Minister Ministre AAFCAAC AAFC: Contact the Minister Do you love nature Because if you do perhaps you could consider that honey bees are dying off it's rare to see a butterfly in Barrie where I live and the once radiant dawn chorus is down to a few last brave tough birds mostly starlings with their horrible song. Oh the ravens are still around. Cah . Please put a ban on Round Up and all Neonicotinoids. They are killing our precious heritage. When I grew up in Mississauga the monarchs were so pervasive they used to get hit by cars or die off on their journey south. I walked a path strewn with butterflies or saw them flitting and wafting en masse in the garden What a change. Now I am lucky if I see ONE OR TWO per year despite gardening with flowers they love and living near farms and fields. Honey Bees Well I have seen one this year so far. They are also getting wiped out.     \"\"\"\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clas = load_data(path, 'data_clas.pkl', bs=bs)\n",
    "learn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5)\n",
    "learn.load('third')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "losses,idxs = interp.top_losses()\n",
    "len(data_clas.valid_ds)==len(losses)==len(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.plot_confusion_matrix(figsize=(10,10), dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2arr(l):\n",
    "    \"Convert list into pytorch Variable.\"\n",
    "    return torch.LongTensor(np.expand_dims(np.array(l), -1)).cuda()\n",
    "\n",
    "def make_prediction_from_list(model, l):\n",
    "    \"\"\"\n",
    "    Encode a list of integers that represent a sequence of tokens.  The\n",
    "    purpose is to encode a sentence or phrase.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    model : fastai language model\n",
    "    l : list\n",
    "        list of integers, representing a sequence of tokens that you want to encode\n",
    "\n",
    "    \"\"\"\n",
    "    arr = list2arr(l)# turn list into pytorch Variable with bs=1\n",
    "    model.reset()  # language model is stateful, so you must reset upon each prediction\n",
    "    hidden_states = model(arr)[-1][-1] # RNN Hidden Layer output is last output, and only need the last layer\n",
    "\n",
    "    #return avg-pooling, max-pooling, and last hidden state\n",
    "    return hidden_states.mean(0), hidden_states.max(0)[0], hidden_states[-1]\n",
    "\n",
    "def getDocumentEmbedding(txt, model):\n",
    "    x,y = data_lm.one_item(txt)\n",
    "    avg_hs, max_hs, last_hs = make_prediction_from_list(model, x[0])\n",
    "    v_s = (avg_hs[0]).cpu().detach().numpy()\n",
    "    return (v_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = [d[0].data for d in data_clas.train_ds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hs = []\n",
    "targets = []\n",
    "for d in ds:\n",
    "    avg_hs, max_hs, last_hs = make_prediction_from_list(learn.model, d)\n",
    "    df_hs.append(avg_hs.cpu().detach().numpy()[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(pd.DataFrame(df_hs))\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "principalDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8,8))\n",
    "ax = fig.add_subplot(1,1,1) \n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "ax.scatter(principalDf['principal component 1']\n",
    "               , principalDf['principal component 2']\n",
    "               , s = 50)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.summarization.summarizer import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = df.text[5]\n",
    "print(txt)\n",
    "print(\"--------------------------------------------\")\n",
    "print(summarize(txt, word_count=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = df.text[100]\n",
    "\n",
    "e = re.sub('\\n+','\\n',re.sub('\\.+','.',re.sub('[^A-Za-z0-9;:. \\n\\']+', '', text)))\n",
    "print(summarize(e, word_count=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai)",
   "language": "python",
   "name": "wsfastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}


#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: dev_nb/S01_ETM_MODEL.ipynb

from __future__ import print_function

import torch
import pickle
import numpy as np
import os
import math
import random
import sys
import matplotlib.pyplot as plt
import lib.ETM_Databunch as db
import scipy.io

from torch import nn, optim
from torch.nn import functional as F
from lib.ETM_model import ETM
from lib.ETM_utils import nearest_neighbors, get_topic_coherence, get_topic_diversity

import math
import scipy
from lib.ETM_model import ETM
from shutil import copyfile

import pickle 
from collections import Counter
from gensim.utils import simple_preprocess

import pandas as pd

def upper_tri_masking(A):
    m = A.shape[0]
    r = np.arange(m)
    mask = r[:,None] < r
    return A[mask]

def load(filename = "./models/Learner.mdl"):
    filehandler = open(filename, 'rb') 
    Learner = pickle.load(filehandler)
    filehandler.close()    
    return Learner

def save(Learner, filename = "./models/Learner.mdl"):
    filehandler = open(filename, 'wb') 
    pickle.dump(Learner, filehandler)
    filehandler.close()  

def cut_range(data, len_s=400):
    '''
    procedures to cut the index of words 
    Usage: cut_range(data_words, 400)
    '''
    nbatch = int(len(data)/len_s)
    li = ([(i*len_s,  i*len_s + len_s) for i in range(nbatch+1)] +  
        [(i*len_s + int(len_s/2),  i*len_s + int(len_s*1.5)) for i in range(nbatch)])
    li.sort()
    if len(li) > 2: 
        li = li[:-1]
    return li

def report_Results(Learner): 
    topics = []
    datawords = [[w for w in simple_preprocess(str(words), deacc=True)] for words in Learner.databunch.texts["test"]]
    for d in datawords:
        texts = []
        for start, end in cut_range(d, Learner.databunch.batch_wd_count):
            texts.append(" ".join(d[start:end]))
        topics.append(Learner.predict_data_batch(Learner.databunch.text2DataBatch(texts)))
    df_all = pd.DataFrame({"TXT": Learner.databunch.texts["test"], "topics": topics})
    return df_all

def archiveModel(rpt_key, Learner):    
    d_rpt = './/models//'+ rpt_key
    f_mdl = d_rpt + "//Learner_" + rpt_key + ".mdl"
    if not os.path.exists(d_rpt):
        os.makedirs(d_rpt)
        copyfile(".//models//Learner.mdl", f_mdl)
    df_all = report_Results(Learner)

    flatten = lambda l: [item for sublist in l for item in sublist]

    if not os.path.exists(d_rpt+"//log"):
        os.makedirs(d_rpt+"//log")

    flog = d_rpt+"//log//rpt_"+ rpt_key + ".log"
    fp = open(flog, "w")
    fp.write("Model Parameters \n")
    fp.write("++++++++++++++++ \n")
    fp.write("Name: "+ rpt_key + "\n")
    fp.write("Data Source: " + Learner.databunch.name + "\n")
    fp.write("num of topics: " + str(Learner.num_topics) + "\n" )
    fp.write("num of vocabs: " + str(len(Learner.databunch.vocab)) + "\n" )
    fp.write("clientVocabWT: " + str(Learner.databunch.clientVocabWT) + "\n" )
    fp.write("isTf: " + str(Learner.databunch.isTf) + "\n" )
    fp.write("isIDF: " + str(Learner.databunch.isIDF) + "\n" )
    fp.write("num of hidden size: " + str(Learner.hidden_size) + "\n")
    fp.write("theta_act: " + str(Learner.theta_act) + "\n" )
    fp.write("rho_size: " + str(Learner.rho_size) + "\n")
    fp.write("min_term_freq: " + str(Learner.databunch.min_term_freq) + "\n" )
    fp.write("max_term_potion: " + str(Learner.databunch.max_term_potion) + "\n")
    fp.write("min_doc_length: " + str(Learner.databunch.min_doc_length) + "\n")
    fp.write("skipCycle: " + str(Learner.skipCycle) + "\n" )
    fp.write("shrink_idv: " + str(Learner.shrink_idv) + "\n")
    fp.write("shrink_lg: " + str(Learner.shrink_lg) + "\n" )

    fp.write("Model Architecture \n")
    fp.write("++++++++++++++++++ \n")
    fp.write(str(Learner.model) + "\n")

    fp.write("Topic Keywords \n")
    fp.write("++++++++++++++ \n")
    Learner.num_words = 20
    topics = Learner.visualize(show=False)
    c = Counter(flatten([set(e) for e in df_all['topics']]))
    df_topics = pd.DataFrame({"keywords": topics, "count": [c[i] for i in range(len(topics))]})

    for (i, keywords, count) in zip(df_topics.index, df_topics["keywords"], df_topics['count']):
        fp.write ("Topic_" + str(i) + ", n:" + str(count) + ", >>: " + ",".join(keywords.split(" ")) + "\n")

    fp.write("Models variable \n")
    fp.write("+++++++++++++++ \n")
    df_k = pd.DataFrame({"rho": Learner.model.rho.weight.flatten().cpu().detach().numpy()})
    fp.write(str(df_k.describe(percentiles=[.01, .10, .25, .5, .75, .90, .99])) + "\n")
    fp.write("--------------------------- \n")
    df_k = pd.DataFrame({"alphas": Learner.model.alphas.weight.flatten().cpu().detach().numpy()})
    fp.write(str(df_k.describe(percentiles=[.01, .10, .25, .5, .75, .90, .99])) + "\n")
    fp.write("--------------------------- \n")
    df_k = pd.DataFrame({"q_theta_0": Learner.model.q_theta[0].weight.flatten().cpu().detach().numpy()})
    fp.write(str(df_k.describe(percentiles=[.01, .10, .25, .5, .75, .90, .99])) + "\n")
    fp.write("--------------------------- \n")
    df_k = pd.DataFrame({"q_theta_0_b": Learner.model.q_theta[0].bias.flatten().cpu().detach().numpy()})
    fp.write(str(df_k.describe(percentiles=[.01, .10, .25, .5, .75, .90, .99])) + "\n")
    fp.write("--------------------------- \n")
    df_k = pd.DataFrame({"q_theta_0_b": Learner.model.q_theta[0].bias.flatten().cpu().detach().numpy()})
    fp.write(str(df_k.describe(percentiles=[.01, .10, .25, .5, .75, .90, .99])) + "\n")
    fp.write("--------------------------- \n")
    df_k = pd.DataFrame({"q_theta_2": Learner.model.q_theta[2].weight.flatten().cpu().detach().numpy()})
    fp.write(str(df_k.describe(percentiles=[.01, .10, .25, .5, .75, .90, .99])) + "\n")
    fp.write("--------------------------- \n")
    df_k = pd.DataFrame({"q_theta_2_b": Learner.model.q_theta[2].bias.flatten().cpu().detach().numpy()})
    fp.write(str(df_k.describe(percentiles=[.01, .10, .25, .5, .75, .90, .99])) + "\n")
    fp.write("--------------------------- \n")
    df_k = pd.DataFrame({"mu_q_theta": Learner.model.mu_q_theta.weight.flatten().cpu().detach().numpy()})
    fp.write(str(df_k.describe(percentiles=[.01, .10, .25, .5, .75, .90, .99])) + "\n")
    fp.write("--------------------------- \n")
    df_k = pd.DataFrame({"mu_q_theta_b": Learner.model.mu_q_theta.bias.flatten().cpu().detach().numpy()})
    fp.write(str(df_k.describe(percentiles=[.01, .10, .25, .5, .75, .90, .99])) + "\n")
    fp.write("--------------------------- \n")
    df_k = pd.DataFrame({"logsigma_q_theta": Learner.model.logsigma_q_theta.weight.flatten().cpu().detach().numpy()})
    fp.write(str(df_k.describe(percentiles=[.01, .10, .25, .5, .75, .90, .99])) + "\n")
    fp.write("--------------------------- \n")
    df_k = pd.DataFrame({"logsigma_q_theta_b": Learner.model.logsigma_q_theta.bias.flatten().cpu().detach().numpy()})
    fp.write(str(df_k.describe(percentiles=[.01, .10, .25, .5, .75, .90, .99])) + "\n")

    n = max(flatten(df_all['topics']))
    a = np.zeros([len(df_all), n+1], dtype = int)
    i = 0
    for e in df_all['topics']:   
        for j in e:
             a[i,j] = 1  
        i = i + 1
    coocc = np.dot(a.transpose(),a)
    for i in range(n+1):
        buf = ""
        for j in range(n+1):
            buf = buf + str(coocc[i,j]) + ","
        fp.write (buf + "\n")
    fp.close()
    return flog

    
class ETM_Topic_Modeling_Learner():
    def __init__(self, databunch, save_path = './results', device = torch.device("cuda" if torch.cuda.is_available() else "cpu")):
        # per instance settings
        self.save_path = save_path;
        self.databunch = databunch;
        self.vocab_size = len(self.databunch.vocab)
        self.batch_size = self.databunch.batch_size
        # LDA specific word embeddings
        self.embeddings = None;
        self.train_embeddings = 1;
        # model architecture attributes
        self.hidden_size = 800;
        self.rho_size = 300; # word embedding
        self.emb_size = 300; # topic embedding
        self.theta_act = "relu";# activation
        self.enc_drop = 0.6;
        self.wdecay = 1.2e-5;
        self.lr = 0.005;
        self.clip = 0;
        # display attributes
        self.log_interval = 30;
        self.visualize_every = 200;
        self.num_words = 20;

        self.skipCycle = 200
        self.shrink_idv = 0.98
        self.shrink_lg = 0.35

        self.device = device;

    def __call__(self, num_topics=20):
        self.num_topics = num_topics;
        self.model = ETM(self.num_topics, self.vocab_size, self.hidden_size, self.rho_size, self.emb_size, self.theta_act, self.embeddings, self.train_embeddings, self.enc_drop).to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.wdecay)

    def calcNormalized_data_batch(self, data_batch_1):
        sums_1 = data_batch_1.sum(1).unsqueeze(1)
        normalized_data_batch_1 = data_batch_1
        return normalized_data_batch_1

    def fit(self, epoch, isDisplay=False):
        acc_loss = 0
        acc_kl_theta_loss = 0
        acc_r_sp_loss = 0
        cnt = 0
        for idx in range(self.databunch.nBatch['train']):
            self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=self.wdecay)
            self.optimizer.zero_grad()
            self.model.zero_grad()

            data_batch = self.databunch.get_batch(idx, datasrc="train")
            normalized_data_batch = self.calcNormalized_data_batch(data_batch)
            recon_loss, kld_theta = self.model(data_batch, normalized_data_batch)

            x = self.model.mu_q_theta.weight
            y = self.model.mu_q_theta.bias
#            zx = x.clone().cpu().detach().numpy()
#            zy = y.clone().cpu().detach().numpy()
#            za = np.vstack((zx.transpose(),zy)).transpose()
#            X = self.model.get_beta().cpu().detach().numpy()
#            D = scipy.spatial.distance.cdist(za,za)
#            P = upper_tri_masking(D)
#            u_p = sum(P)/len(P)
#            r_sp = torch.from_numpy(np.array((u_p/min(P) + 6/u_p)**0.5/10)).float().cuda()

            total_loss = recon_loss + kld_theta
        #+ r_sp
            total_loss.backward()
            if self.clip > 0:
                torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
            self.optimizer.step()
            acc_loss += torch.sum(recon_loss).item()
            acc_kl_theta_loss += torch.sum(kld_theta).item()
#            acc_r_sp_loss += torch.sum(r_sp).item()
            cnt += 1
            if idx % self.log_interval == 0 and idx >= 0:
                cur_loss = round(acc_loss / cnt, 2)
                cur_kl_theta = round(acc_kl_theta_loss / cnt, 2)
                cur_r_sp = round(acc_r_sp_loss / cnt, 2)
                cur_real_loss = round(cur_loss + cur_kl_theta, 2)
                cur_LR = round(self.optimizer.param_groups[0]['lr'] * 1000, 2)
                if isDisplay:
                    print('Epoch: {} .. batch: {}/{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {} ..'.format(
                        epoch, idx, self.databunch.nBatch['train']-1, cur_LR, cur_kl_theta, cur_loss, cur_real_loss))
        cur_loss = round(acc_loss / cnt, 2)
        cur_kl_theta = round(acc_kl_theta_loss / cnt, 2)
#        cur_r_sp = round(acc_r_sp_loss / cnt, 2)
        cur_real_loss = round(cur_loss + cur_kl_theta, 2)
        if isDisplay:
            print('*'*100)
            print('Epoch----->{} .. LR: {} .. KL_theta: {} .. Rec_loss: {} .. NELBO: {} ..'.format(
                    epoch, round(self.optimizer.param_groups[0]['lr'] * 1000, 2), cur_kl_theta, cur_loss, cur_real_loss))

    def evaluate(self, source, tc=False, td=False, isDisplay=False):
        """Compute perplexity on document completion.
        """
        m = self.model
        m.eval()
        with torch.no_grad():
            beta = m.get_beta()
            ### do dc and tc here
            acc_loss = 0
            cnt = 0
            if (source=="train"):
                nBatch = self.databunch.nBatch['train']
            elif (source=="test"):
                nBatch = self.databunch.nBatch['test']
            elif ("valid" in source):
                nBatch = self.databunch.nBatch['valid']
            else:
                pass
            for idx in range(nBatch):
                ## get theta from first half of docs
                data_batch_1 = self.databunch.get_batch(idx, datasrc="valid1")
                normalized_data_batch_1 = self.calcNormalized_data_batch(data_batch_1)
                theta, _ = m.get_theta(normalized_data_batch_1, 0)
                ## get prediction loss using second half
                data_batch_2 = self.databunch.get_batch(idx, datasrc="valid2")
                sums_2 = data_batch_2.sum(1).unsqueeze(1)

                res = torch.mm(theta, beta)
                preds = torch.log(res)
                recon_loss = -(preds * data_batch_2).sum(1)
                loss = recon_loss / sums_2.squeeze()
                loss = loss.mean().item()
                if not math.isnan(loss):
                    acc_loss += loss
                    cnt += 1

            cur_loss = acc_loss / cnt
            ppl_dc = round(math.exp(cur_loss), 1)
            if isDisplay:
                print('*'*100)
                print('{} Doc Completion PPL: {}'.format(source.upper(), ppl_dc))
                print('*'*100)
                if tc or td:
                    beta = beta.data.cpu().numpy()
                    if tc:
                        print('Computing topic coherence...')
                        get_topic_coherence(beta, train_tokens, vocab)
                    if td:
                        print('Computing topic diversity...')
                        get_topic_diversity(beta, 25)
            return ppl_dc

    def visualize(self, show=True):
        m = self.model
        if not os.path.exists('./results'):
            os.makedirs('./results')
        m.eval()
        ## visualize topics using monte carlo
        with torch.no_grad():
            if show:
                print('#'*100)
                print('Visualize topics...')
            topics_words = []
            gammas = m.get_beta()
            for k in range(self.num_topics):
                gamma = gammas[k]
                top_words = list(gamma.cpu().numpy().argsort()[-self.num_words+1:][::-1])
                topic_words = [self.databunch.vocab[a] for a in top_words]
                topics_words.append(' '.join(topic_words))
                if show:
                    print('T_{}:{}'.format(k, topic_words))
        return topics_words

    def fit_one_cycle(self, num_epoch=100):
        best_epoch = 0
        best_val_ppl = 1e9

        # annealing parameter
        skip = self.skipCycle
        if num_epoch < skip: skip = num_epoch
        shrink_idv = self.shrink_idv
        shrink_lg = self.shrink_lg

        all_val_ppls = []
        ckpt = os.path.join(self.save_path,
                'etm_{}_K_{}_Htheta_{}_Optim_{}_Clip_{}_ThetaAct_{}_Lr_{}_Bsz_{}_RhoSize_{}_trainEmbeddings_{}'.format(
                self.databunch.name, self.num_topics, self.hidden_size, "adam", self.clip, self.theta_act,
                    self.lr, self.batch_size['train'], self.rho_size, self.train_embeddings))
        lr = self.lr
        for epoch in range(1, num_epoch+1):
            if (epoch % skip) == (skip-1):
                self.lr = self.lr * math.floor((shrink_lg**(skip/num_epoch))/(shrink_idv**skip))
            else:
                self.lr = self.lr * shrink_idv
            isDisplay = (epoch % int(num_epoch/30) == 0)
            self.fit(epoch, isDisplay)
            val_ppl = self.evaluate('valid', isDisplay=isDisplay)
            if val_ppl < best_val_ppl:
                with open(ckpt, 'wb') as f:
                    torch.save(self.model, f)
                best_epoch = epoch
                best_val_ppl = val_ppl
            else:
                ## check whether to anneal lr
                lr = self.optimizer.param_groups[0]['lr']
            if epoch % self.visualize_every == 0:
                self.visualize()
            all_val_ppls.append(val_ppl)
        with open(ckpt, 'rb') as f:
            model = torch.load(f)
        model = model.to(self.device)
        val_ppl = self.evaluate('valid',True)
        self.visualize()

    def predict_data_batch(self, data_batch):
        '''
        prediction of topics
        '''
        sums = data_batch.sum(1).unsqueeze(1)

        normalized_data_batch = data_batch
        theta, _ = self.model.get_theta(normalized_data_batch,enc_drop =0)
        arr2D = theta.detach().cpu().numpy()
        arr_topic = list(arr2D.argmax(axis = 1))
        return  arr_topic
	

	